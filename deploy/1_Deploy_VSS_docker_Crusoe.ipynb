{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA AI Blueprint for Video Search and Summarization: Docker Deployment\n",
    "\n",
    "This notebook will go over the steps to deploy the NVIDIA AI Blueprint for Video Search and Summarization which can ingest massive volumes of live or archived videos and extract insights for summarization and interactive Q&A.\n",
    "\n",
    "We will go over the docker compose deployment steps which as an alternative to the helm chart deployment. As this blueprint uses multiple models including a VLM, LLM, Reranker, Embedding model, there are multiple ways to deploy the blueprint based on the model deployment (Self-hosted or NV-hosted /w API Key). Here we will be running all models locally/self-hosted.\n",
    "\n",
    "**Note**: this notebook is designed to run as a [brev.dev launchable](https://console.brev.dev/launchable/deploy/?launchableID=env-2olGFXbhH0qEtU47MJ4tpxCAZIn) on 8XL40S GPU **(Specifically CRUSOE Cloud Provider with Ephemeral storage)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Obtain NVIDIA API Keys\n",
    "\n",
    "This key will be used to pull relevant models and containers from build.nvidia.com and NGC.\n",
    "\n",
    "Generate the key from [NGC Portal](https://ngc.nvidia.com/) using the same account used to apply for [Blueprint Early Access.](https://developer.nvidia.com/ai-blueprint-for-video-search-and-summarization-early-access/join)\n",
    "\n",
    "Follow the instructions to [Generate NGC API Key.](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#generating-api-key)\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b>  If you have authentication issues when pulling the NIMs, please verify you have the following <a href=\"https://org.ngc.nvidia.com/subscriptions\" target=\"_blank\">Subscriptions</a>: <strong>NVIDIA Developer Program</strong>\n",
    " </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"NGC_API_KEY\"] = \"***\" #Replace with your key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the Vision Language Model (VLM)\n",
    "\n",
    "By default, we are using [VILA-1.5](https://build.nvidia.com/nvidia/vila) model. You could use other models like [NVILA](https://huggingface.co/Efficient-Large-Model/NVILA-15B), GPT-4o, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VLM_MODEL_TO_USE\"] = \"vila-1.5\" #or choose nvila\n",
    "os.environ[\"MODEL_PATH\"] = \"ngc:nim/nvidia/vila-1.5-40b:vila-yi-34b-siglip-stage3_1003_video_v8\" #for nvila, use \"git:https://huggingface.co/Efficient-Large-Model/NVILA-15B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring the user is on right path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/NVIDIA-AI-Blueprints/video-search-and-summarization.git\n",
    "# %cd video-search-and-summarization/docker/launchables\n",
    "\n",
    "%cd ./docker/launchables\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the Driver and CUDA version to be the following:\n",
    "- Driver Version: 535.x.x\n",
    "- CUDA Version 12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the driver version doesn't match in the above step:\n",
    "- Update the Driver to 535\n",
    "- Reboot the system\n",
    "- Set ```NGC_API_KEY``` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt install nvidia-driver-535 -y\n",
    "# !sudo reboot now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment: Using all self-hosted models\n",
    "\n",
    "We will be using Cosmos Nemotron VLM, which is part of the main container. All other models need to be set up before proceeding with the blueprint container. These include:\n",
    "- [Embedding NIM](https://build.nvidia.com/nvidia/llama-3_2-nv-embedqa-1b-v2)\n",
    "- [Reranker NIM](https://build.nvidia.com/nvidia/llama-3_2-nv-rerankqa-1b-v2)\n",
    "- [LLM NIM](https://build.nvidia.com/meta/llama-3_1-70b-instruct)\n",
    "\n",
    "\n",
    "### GPU Configuration\n",
    "\n",
    "![VSS GPU Config](images/vss_gpu_layout.png)\n",
    "\n",
    "In order to update the GPUs used by each model:\n",
    "- **LLM, Embedding and Reranking models:** Update the ```--gpus``` parameters while deploying NIMs in Steps 2-4 below.\n",
    "- **VLM:** Update ```NVIDIA_VISIBLE_DEVICES``` in docker/.env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set Environment Variables and Login to Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"LOCAL_NIM_CACHE\"] = os.path.expanduser(\"~/.cache/nim\") #default cache location\n",
    "os.environ[\"LOCAL_NIM_CACHE\"] = os.path.expanduser(\"/ephemeral/cache/nim\") #updating with ephemeral storage\n",
    "os.makedirs(os.environ[\"LOCAL_NIM_CACHE\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the docker storage path to Ephemeral storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, subprocess, time\n",
    "\n",
    "storage_path = \"/ephemeral/cache/docker\"\n",
    "\n",
    "daemon_file = \"/etc/docker/daemon.json\" #update the path if required\n",
    "config = {}\n",
    "try:\n",
    "    config = json.load(open(daemon_file)) if os.path.exists(daemon_file) else {}\n",
    "except PermissionError:\n",
    "    print(\"Cannot read the file. Try running with elevated privileges or check docker deamon file path.\")\n",
    "\n",
    "config[\"data-root\"] = storage_path\n",
    "config_str = json.dumps(config, indent=4)\n",
    "\n",
    "subprocess.run(f\"echo '{config_str}' | sudo tee {daemon_file} > /dev/null\", shell=True, check=True)\n",
    "subprocess.run(\"sudo systemctl restart docker\", shell=True, check=True)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Verify new storage location\n",
    "print(subprocess.run(\"docker info | grep 'Docker Root Dir'\", shell=True, capture_output=True, text=True).stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Launch the LLM NIM.\n",
    "Note: If you're logged in as root, make a separate llm_user account and give it permission to the nim cache folder.\n",
    "\n",
    "Here, we have preset the GPUs to use ```--gpus``` and port ```-p``` for the most optimal deployment on 8xL40s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    "    --gpus '\"device=0,1,2,3\"' \\\n",
    "    --shm-size=16GB \\\n",
    "    -e NGC_API_KEY \\\n",
    "    -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "    -u $(id -u) \\\n",
    "    -p 8000:8000 \\\n",
    "    -d \\\n",
    "    nvcr.io/nim/meta/llama-3.1-70b-instruct:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Launch the Reranker NIM. \n",
    "\n",
    "Here we have preset the GPUs to use ```--gpus``` and port ```-p``` for the most optimal deployment on 8xL40s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    "    --gpus '\"device=4\"' \\\n",
    "    --shm-size=16GB \\\n",
    "    -e NGC_API_KEY \\\n",
    "    -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "    -u $(id -u) \\\n",
    "    -p 9235:8000 \\\n",
    "    -d \\\n",
    "    nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Launch the Embedding NIM.\n",
    "\n",
    "Again, we have preset the GPUs to use ```--gpus``` and port ```-p``` for the most optimal deployment on 8xL40s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    "    --gpus '\"device=5\"' \\\n",
    "    --shm-size=16GB \\\n",
    "    -e NGC_API_KEY \\\n",
    "    -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "    -u $(id -u) \\\n",
    "    -p 9234:8000 \\\n",
    "    -d \\\n",
    "    nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Verify all 3 NIMs are running\n",
    "\n",
    "After running the following cell, you should be able to see three containers, one for each model.\n",
    "\n",
    "![Container List](images/containers_pre_check.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell ensures that the LLM NIM is running before proceeding.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b>  LLM NIM service could take a couple of miniutes to get ready.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:8000/v1/health/ready' #make sure the LLM NIM port is correct\n",
    "headers = {'accept': 'application/json'}\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data.get(\"message\") == \"Service is ready.\":\n",
    "                print(\"Service is ready.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Service is not ready. Waiting for 30 seconds...\")\n",
    "        else:\n",
    "            print(f\"Unexpected status code {response.status_code}. Waiting for 30 seconds...\")\n",
    "    except requests.ConnectionError:\n",
    "        print(\"Service is not ready. Waiting for 30 seconds...\")\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Launch the Blueprint\n",
    "\n",
    "Before proceeding, make sure you have compose.yaml, config.yaml and .env in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update docker compose version (recommended v2.32.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose version\n",
    "\n",
    "!mkdir -p ~/.docker/cli-plugins\n",
    "!curl -SL https://github.com/docker/compose/releases/latest/download/docker-compose-linux-x86_64 -o ~/.docker/cli-plugins/docker-compose\n",
    "!chmod +x ~/.docker/cli-plugins/docker-compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Docker Compose Deployment**\n",
    "\n",
    "Here, we start the docker container to spin up the blueprint. The container performs the following main steps:\n",
    "1. Downloads the VLM\n",
    "2. Performs model calibration\n",
    "3. Generates a TRT-LLM Engine\n",
    "4. Spins up the Milvus and Neo4J database.\n",
    "5. Starts Video Search and Summarization service\n",
    "6. Finally, we get the frontend and backend endpoints\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b>  This step can take around 30 minutes for the first run as it goes through Steps 1-3 mentioned above\n",
    " </div>\n",
    "\n",
    "The below code cell filters out important logs so that the output is not very verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!docker compose down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "keywords = [\"Milvus server started\", \"Downloading model\", \"Downloaded model\", \"VILA Embeddings\", \"VILA TRT model load execution time\", \n",
    "            \"Starting quantization\", \"Quantization done\", \"Engine generation completed\", \"TRT engines generated\", \"Uvicorn\", \n",
    "            \"VIA Server loaded\", \"Backend\", \"Frontend\", \"****\"]\n",
    "\n",
    "# Start the docker compose process in detached mode\n",
    "subprocess.run(['docker', 'compose', 'up', '--quiet-pull', '-d'])\n",
    "\n",
    "def filter_logs(logs, keywords):\n",
    "    return [line for line in logs.splitlines() if any(keyword in line for keyword in keywords)]\n",
    "\n",
    "printed_lines = set()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        logs = subprocess.check_output(['docker', 'compose', 'logs', '--no-color'], universal_newlines=True)\n",
    "        filtered_logs = filter_logs(logs, keywords)\n",
    "        new_logs = [line for line in filtered_logs if line not in printed_lines]\n",
    "        \n",
    "        for line in new_logs:\n",
    "            print(line)\n",
    "            printed_lines.add(line)\n",
    "            if \"Frontend\" in line:\n",
    "                print(\"VSS Server ran successfully.\")\n",
    "                print(\"Access VSS Frontend UI from Brev portal tunnels section. Refer to Step 7 for more details.\")\n",
    "                raise SystemExit\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping log tailing...\")\n",
    "except SystemExit:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Access VSS UI with Brev Tunnels\n",
    "\n",
    "1. Go to the \"Access\" Tab on Brev and scroll down to the \"Using Tunnels\" section   \n",
    "<img src=\"images/brev_access_tab.png\" alt=\"Access Tab\" width=\"1200\"/>\n",
    "\n",
    "2. (Optional) Add port \"9100\". This is set in .env file which is configurable   \n",
    "<img src=\"images/brev_add_port.png\" alt=\"Access Tab\" width=\"1200\"/>\n",
    "\n",
    "3. Click on the frontend port (9100) Sharable URL link to access blueprint UI  \n",
    "    <div class=\"alert alert-block alert-success\">\n",
    "        <b>Note:</b>  Please reload the brev page if the shareable URL is showing as unhealthy or follow the alternative steps with VSCode below.\n",
    "    </div>\n",
    "    <img src=\"images/brev_vss_ui_url.png\" alt=\"Access Tab\" width=\"1200\"/>\n",
    "\n",
    "4. Experience VSS using the gradio based UI application. For quick steps to summarize a video, refer to [this link](https://docs.nvidia.com/vss/latest/content/sample_summarization.html). Additionally, for detailed instructions on how to use the UI, follow [this guide](https://docs.nvidia.com/vss/latest/content/ui_app.html)  \n",
    "<img src=\"images/vss_landing_page.png\" alt=\"Access Tab\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Alertnative Option] Access instance on VSCode and add ports\n",
    "\n",
    "1. First, go to the \"Access\" tab  \n",
    "<img src=\"images/brev_access_tab.png\" alt=\"Access Tab\" width=\"1200\"/>\n",
    "\n",
    "2. Refer to the commands in \"Using Brev CLI\" to open VSCode locally  \n",
    "    <div class=\"alert alert-block alert-success\">\n",
    "        <b>Note:</b>  Make sure to <a href=\"https://code.visualstudio.com/docs/setup/mac#_configure-the-path-with-vs-code\" target=\"_blank\">Configure the path with VS Code</a> if you run into erros while accessing instance through VSCode.\n",
    "    </div>  \n",
    "    <img src=\"images/brev_vscode_access.png\" alt=\"Access Tab\" width=\"1200\"/>\n",
    "\n",
    "3. Navigate to the Ports view in the Panel region (Ports: Focus on Ports View), and select Forward a Port  \n",
    "<img src=\"images/vscode_ports.png\" alt=\"VSCode Ports\" width=\"1200\"/>\n",
    "\n",
    "4. Add frontend (9100) and backend (8100) ports, and access blueprint UI by clicking on the \"localhost:9100\"  \n",
    "<img src=\"images/vscode_access_vss_ui.png\" alt=\"VSCode Ports\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uninstalling the blueprint\n",
    "\n",
    "Uncomment the following cell to stop the blueprint container, followed by stopping all model containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # To bring down the blueprint instance\n",
    "# !docker compose down\n",
    "\n",
    "# # To stop all other containers\n",
    "# !docker stop $(docker ps -q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
